# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Useful Resources
- [ScriptRunConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.scriptrunconfig?view=azure-ml-py)
- [Configure and submit training runs](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-set-up-training-targets)
- [HyperDriveConfig Class](https://docs.microsoft.com/en-us/python/api/azureml-train-core/azureml.train.hyperdrive.hyperdriveconfig?view=azure-ml-py)
- [How to tune hyperparamters](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters)


## Summary
**Explain the problem statement: e.g "This dataset contains data about... we seek to predict..."**
The [dataset](https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv) contains information about bank customers and marketing campaigns. It includes features like customer demographics and campaign details. The goal is to use a classification model to predict whether a customer will subscribe to a term deposit based on this data. The target variable (y) indicates whether the customer has already subscribed to a term deposit.

**Explain the solution: e.g. "The best performing model was a ..."**
The AutoML process evaluated various classification models and configurations to determine the best-performing model for predicting term deposit subscriptions. The primary metric used for evaluation was `Accuracy`

The `VotingEnsemble` model emerged as the top performer, outperforming other models like `LightGBM` and `XGBoostClassifier`. This suggests that combining the predictions from multiple models through a voting mechanism effectively improved the overall predictive accuracy.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
- After downloading the data, a cleaning and preprocessing operation was performed, which involved handling missing values, converting categorical variables into numerical formats, and mapping specific string values to integers.  
- Once the data was cleaned, it was split into training and testing sets for model evaluation.

**What are the benefits of the parameter sampler you chose?**
- The `RandomParameterSampling` method was chosen to explore hyperparameters such as `inverse regularization strength (C)` ranging from 0.1 to 10 and `max iterations` with values of 50, 75, and 100. Unlike an exhaustive grid search, this method randomly selects a subset of hyperparameters, which significantly reduces the computational time and cost, making the process more efficient.

**What are the benefits of the early stopping policy you chose?**
- The `BanditPolicy` was used for early stopping, which helps conserve computational resources by terminating runs that are underperforming. If a runs performance is 10% worse than the best-performing run during intermediate evaluations, it is stopped early to avoid unnecessary computation.

**List of runs of Logistic model**
![list_runs_logistic_model](/assets/logistic_with_hyperdrive_runs.png)

**Compare runs of Logistic model**
![compare_runs_logistic_model](/assets/logistic_with_hyperdrive_chart.png)

## AutoML
**Describe the model and hyperparameters generated by AutoML.**

| ITER | PIPELINE                                       | DURATION  | METRIC | BEST   |
|------|------------------------------------------------|-----------|--------|--------|
| 0    | MaxAbsScaler LightGBM                          | 0:00:13   | 0.9135 | 0.9135 |
| 1    | MaxAbsScaler XGBoostClassifier                 | 0:00:26   | 0.9118 | 0.9135 |
| 2    | MaxAbsScaler ExtremeRandomTrees                | 0:00:15   | 0.7294 | 0.9135 |
| 3    | SparseNormalizer XGBoostClassifier             | 0:00:15   | 0.9131 | 0.9135 |
| 4    | MaxAbsScaler LightGBM                          | 0:00:09   | 0.9124 | 0.9135 |
| 5    | MaxAbsScaler LightGBM                          | 0:00:09   | 0.8884 | 0.9135 |
| 6    | StandardScalerWrapper XGBoostClassifier        | 0:00:12   | 0.9090 | 0.9135 |
| 7    | MaxAbsScaler LogisticRegression                | 0:00:10   | 0.9089 | 0.9135 |
| 8    | StandardScalerWrapper ExtremeRandomTrees       | 0:00:10   | 0.8882 | 0.9135 |
| 9    | StandardScalerWrapper XGBoostClassifier        | 0:00:10   | 0.9148 | 0.9148 |
| 10   | SparseNormalizer LightGBM                      | 0:00:09   | 0.9046 | 0.9148 |
| 11   | StandardScalerWrapper XGBoostClassifier        | 0:00:10   | 0.9126 | 0.9148 |
| 12   | MaxAbsScaler LogisticRegression                | 0:00:12   | 0.9101 | 0.9148 |
| 13   | MaxAbsScaler SGD                               | 0:00:09   | 0.8516 | 0.9148 |
| 14   | StandardScalerWrapper XGBoostClassifier        | 0:00:12   | 0.9124 | 0.9148 |
| 15   | SparseNormalizer RandomForest                  | 0:00:29   | 0.8171 | 0.9148 |
| 16   | StandardScalerWrapper LogisticRegression       | 0:00:10   | 0.9089 | 0.9148 |
| 17   | StandardScalerWrapper RandomForest             | 0:00:18   | 0.8996 | 0.9148 |
| 18   | StandardScalerWrapper XGBoostClassifier        | 0:00:15   | 0.9134 | 0.9148 |
| 19   | TruncatedSVDWrapper RandomForest               | 0:02:54   | 0.8188 | 0.9148 |
| 20   | TruncatedSVDWrapper RandomForest               | 0:04:40   | nan    | 0.9148 |
| 21   | VotingEnsemble                                 | 0:00:58   | 0.9170 | 0.9170 |
| 22   | StackEnsemble                                  | 0:01:07   | 0.9154 | 0.9170 |

The AutoML run produced a variety of models with different preprocessing and classifier combinations. The top-performing model is a `VotingEnsemble`, achieving the highest accuracy with a score of **0.9170**. Other strong contenders include `XGBoostClassifier` paired with `StandardScalerWrapper` and `MaxAbsScaler`, with accuracy scores close to the best-performing model.

Throughout the iterations, common preprocessing techniques like `MaxAbsScaler`, `SparseNormalizer`, and `StandardScalerWrapper` were applied. Classifiers such as `LightGBM`, `XGBoostClassifier`, and `RandomForest` were evaluated with various configurations. The ensemble models (`VotingEnsemble` and `StackEnsemble`) typically outperformed individual models due to their ability to combine predictions from multiple pipelines.

The hyperparameters tuned in these models include learning rates, max depth, and feature scaling, with `LightGBM` and `XGBoost` showing strong results across different configurations.

**List of runs of AutoML**
![list_runs_automl](/assets/automl_algorithm_runs.png)

**Metrics of best run for AutoML**
![automl_best_run_metrics](/assets/automl_best_run_metrics.png)

**Features importance of best run for AutoML**
![automl_best_run_features_importance](/assets/automl_best_run_features_importance.png)


## Pipeline comparison
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**

The manually built logistic regression model achieved an accuracy of **0.91** with the following best configuration: regularization strength (`C`) of **0.2667** and **50** iterations to converge. This model's performance is limited to the basic evaluation metric of accuracy, which is often used but doesn't fully capture the performance of a model, especially for imbalanced datasets or complex prediction tasks.

On the other hand, the AutoML model provides a much more comprehensive set of evaluation metrics. In terms of accuracy, the AutoML model slightly outperforms the manually built logistic regression model with an accuracy of **0.917**. However, beyond accuracy, the AutoML model shows significantly more detailed metrics, such as AUC (Area Under the Curve), precision, recall, F1 scores, and Matthews correlation coefficient. For example, it reports an AUC-weighted score of **0.946**, which indicates strong overall classification performance. It also achieved a high precision score weighted at **0.91** and an F1 score weighted of **0.912**, showing that the AutoML model is not only making correct predictions but also handling false positives and false negatives effectively.

One key difference in architecture is that the manually built model is a simple `LogisticRegression`, while AutoML tested and optimized various algorithms, as seen in the pipeline output from the earlier experiments. AutoML employs an ensemble approach, trying multiple machine learning algorithms such as `LightGBM`, `XGBoost`, and `LogisticRegression`, then uses stacking and voting ensembles to improve performance. This multi-algorithmic approach often leads to better results compared to a single model approach, especially when handling different patterns in the data.

The improvement in performance by AutoML can be attributed to its ability to automatically tune hyperparameters and select the best-performing model from a variety of algorithms. Additionally, AutoML's use of techniques like early stopping, feature scaling, and hyperparameter optimization (such as `RandomParameterSampling` and `BanditPolicy`) allows it to efficiently explore the best settings, while the manual logistic regression model only experimented with regularization strength and iteration count.

In summary, while the manually built `LogisticRegression` model performed decently, AutoML's broader experimentation with models and metrics resulted in a more robust and accurate model. The difference in accuracy, although not huge, highlights the power of AutoML in automating the machine learning process and optimizing models beyond basic hyperparameter tuning.

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**

1. **Feature Engineering**: Enhancing feature selection and creating new features can help the model capture more complex patterns, improving predictive power.

2. **Hyperparameter Optimization**: Utilizing advanced techniques like Bayesian optimization can lead to more efficient searches through the hyperparameter space, potentially resulting in better configurations and performance.

3. **Model Diversity**: Expanding the range of algorithms tested, including ensemble methods and deep learning approaches, could identify models that outperform the current selections, leading to improved accuracy and robustness.

## Proof of cluster clean up
**Image of cluster marked for deletion**
![delete_compute_cluster](/assets/delete_compute_target.png)

